#!/bin/bash -l

<%- gpu = context.node_type.include?("vis") -%>

# Set working directory to home directory
cd "${HOME}"

#
# Launch Fluxbox
#

# Create Fluxbox root or it will override the below init file
(
  umask 077
  mkdir -p "${HOME}/.fluxbox"
)

# Export the module function for the terminal
[[ $(type -t module) == "function" ]] && export -f module

# Start the Fluxbox window manager (it likes to crash on occasion, make it
# persistent)
(
  until fluxbox -display "${DISPLAY}.0" -rc "<%= session.staged_root.join("fluxbox.rc") %>"; do
    echo "Fluxbox crashed with exit code $?. Respawning..." >&2
    sleep 1
  done
) &

#
# Start ANSYS Workbench
#

# Load the required environment
module purge
module load intel/16.0.3 mvapich2/2.2 <%= context.version %><%= " virtualgl" if gpu %>

# Another ANSYS job with the same job name (file) is already running in this
# directory or the file.lock file has not been deleted from an abnormally
# terminated ANSYS run.  To disable this check, set the ANSYS_LOCK environment
# variable to OFF.
export ANSYS_LOCK="OFF"

#
# CFX Related Options
#

<%- unless gpu -%>
# Disable hardware rendering mode
export CUE_GRAPHICS="mesa"
<%- end -%>

# Fix bug when running Intel MPI code on OSC
export I_MPI_PMI_EXTENSIONS=on

# Add custom "OSC MPI" as a start method
export CFX5_START_METHODS_CCL="<%= session.staged_root.join("cfx_assets", "start-methods.ccl") %>"

# Make a hosts file that CFX will use in Parallel Distributed
mkdir -p "${HOME}/.cfx"
export CFX5_HOSTS_CCL="${HOME}/.cfx/hostinfo.${PBS_JOBID}"
NODES=$(tr '\n' ',' < "${PBS_NODEFILE}" | sed 's/,$//g')
touch "${CFX5_HOSTS_CCL}"
cfx5parhosts -add "${NODES}" -user -file "${CFX5_HOSTS_CCL}"

# Only give OSC MPI option to user through GUI
HOSTINFO=$(head -n -2 "${CFX5_HOSTS_CCL}")
cat > "${CFX5_HOSTS_CCL}" << EOL
${HOSTINFO}
    SOLVER STEP CONTROL:
      Runtime Priority = Standard
      MEMORY CONTROL:
        Memory Allocation Factor = 1.0
      END
      PARALLEL ENVIRONMENT:
        Parallel Host List = ${NODES}
        Start Method = OSC MPI Distributed Parallel
      END
    END
  END # EXECUTION CONTROL
END # SIMULATION CONTROL
EOL

#
# Fluent Related Options
#

# Fluent fix so that is uses pbsrsh set in MPI_REMSH by module load
export HPMPI_MPIRUN_FLAGS="-e MPI_REMSH=$MPI_REMSH"

# Launch ANSYS Workbench
<%= "vglrun " if gpu %>runwb2
